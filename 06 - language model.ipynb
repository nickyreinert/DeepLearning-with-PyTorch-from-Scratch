{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26fadf3",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84aa063",
   "metadata": {},
   "source": [
    "## Optional: GDrive connection\n",
    "If you want to run this on Google Colab, you can connect the notebook to your Google Drive to access training data from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3bea7a",
   "metadata": {},
   "source": [
    "## Libraries and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c351b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on `mps`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mmap\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_data_path = \"./\"\n",
    "source_data = \"text.txt\"\n",
    "model_export = \"model.pth\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f'Running on `{device}`')\n",
    "\n",
    "batch_size = 8 # how many blocks per batch\n",
    "block_size = 8 # length of sequence of tokens, how many chars per block\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "embedding_size = 128 # the size of the embedding vectors that each token will be assigned to, like if A = 1, 1 would be encoded to a vector of size 128\n",
    "num_blocks = 4 # amount of blocks\n",
    "num_heads = 4 # amount of heads per blocks, should be bigger as embedding_size, see next line\n",
    "head_size = embedding_size // num_heads # dont change this, defining it here for better readability\n",
    "dropout_rate = 0.2 # how many nodes to drop per each epoch\n",
    "hidden_units_factor = 4 # how many hidden units for the sequential layer stack, using a factor here, multiplied with embedding_size\n",
    "train_split = 0.8\n",
    "\n",
    "losses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b370ca8",
   "metadata": {},
   "source": [
    "## Data\n",
    "I am giving you two options here: Either create a dummy data set with a lot of simple repetitions. This allows the model to learn fast. Or choose a real world example.\n",
    "\n",
    "For this I am using a very **very small dataset** only, as this is for training purposes only. If you are looking for better results, you may refer to e.g. **OpenWebText** or other available data sets. \n",
    "\n",
    "We are going to read the data and split it into two parts: **Training** data and **testing** data. \n",
    "\n",
    "Next we get a list of all used `chars` and create two tables for encoding and decoding(`char_table` and `reverse_char_table`) text into vector of integers. The `vocab_size` shows the amount of different chars in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393be78c",
   "metadata": {},
   "source": [
    "### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90e9f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello World! Hello World! Hello World! \\n\"\n",
    "sentence = \"ABA\\n\"\n",
    "f_dummy_data = open(f\"{source_data_path}/text.txt\", 'w')\n",
    "f_dummy_data.write(sentence * 1000)\n",
    "f_dummy_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ab082",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "516b1cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 4\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "full_text = []\n",
    "source_data_train = f\"train_{source_data}\"\n",
    "source_data_test = f\"test_{source_data}\"\n",
    "\n",
    "f_full_text = open(f\"{source_data_path}/{source_data}\", \"r\")\n",
    "f_training_text = open(f\"{source_data_path}/{source_data_train}\", 'w')\n",
    "f_test_test = open(f\"{source_data_path}/{source_data_test}\", 'w')\n",
    "\n",
    "for line in f_full_text:\n",
    "  r = random.random()\n",
    "  full_text.append(line)\n",
    "  if (0.0 <=  r <= train_split):\n",
    "    f_training_text.write(line)\n",
    "  else:\n",
    "    f_test_test.write(line)\n",
    "\n",
    "f_full_text.close()\n",
    "f_training_text.close()\n",
    "f_test_test.close()\n",
    "\n",
    "chars = sorted(list(set(' '.join(full_text))))\n",
    "char_table = {char:index for index,char in enumerate(chars)}\n",
    "reverse_char_table = {index:char for index,char in enumerate(char_table)}\n",
    "\n",
    "vocab_size = len(chars)\n",
    "del full_text\n",
    "\n",
    "print(f'Vocabulary size is {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205869ed",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3a686",
   "metadata": {},
   "source": [
    "We will create four functions:\n",
    "`encode()` and `decode()` will allow us to convert text to a vector of integers and back. \n",
    "\n",
    "`get_batch` and `get_random_chunk` will randomly read text chunks from our **training** and **testing** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "506ad003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char_table: list, text: str = None):\n",
    "  return [char_table[char] for char in text]\n",
    "\n",
    "def decode(char_table: list, numbers: list = []):\n",
    "  return ''.join([char_table[index] for index in numbers])\n",
    "\n",
    "# memory map for using small snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = f\"{source_data_path}/{source_data_train}\" if split == 'train' else f\"{source_data_path}/{source_data_test}\"\n",
    "    with open(filename, 'r') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size * batch_size)\n",
    "\n",
    "            # Seek to the random position and read data binary\n",
    "            mm.seek(start_pos)\n",
    "            raw_data = mm.read(block_size * batch_size - 1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            data = raw_data.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            # encode to torch tensor using char table\n",
    "            encoded_data = torch.tensor(encode(char_table = char_table, text = data), dtype=torch.long)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    encoded_data = get_random_chunk(split)\n",
    "    random_block_start_indices = torch.randint(len(encoded_data) - block_size, (batch_size,))\n",
    "\n",
    "    input_sequences = torch.stack([encoded_data[index:index + block_size] for index in random_block_start_indices])\n",
    "    target_sequences = torch.stack([encoded_data[index + 1:index + block_size + 1] for index in random_block_start_indices])\n",
    "\n",
    "    return input_sequences.to(device), target_sequences.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279da1e",
   "metadata": {},
   "source": [
    "The following part just tests encoding and decoding feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d93ed8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0, ' ': 1, 'A': 2, 'B': 3}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33eded27",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'H'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello World!\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m encoded_text \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHello World!\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m decode(reverse_char_table, encoded_text)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncoding `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ` to  `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoded_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ` and back to  `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m `\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(char_table, text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(char_table: \u001b[38;5;28mlist\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [char_table[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text]\n",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(char_table: \u001b[38;5;28mlist\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mchar_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'H'"
     ]
    }
   ],
   "source": [
    "text = 'Hello World!'\n",
    "encoded_text = encode(char_table, 'Hello World!')\n",
    "decoded_text = decode(reverse_char_table, encoded_text)\n",
    "print(f'Encoding `{text} ` to  `{encoded_text} ` and back to  `{decoded_text} `')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643755c6",
   "metadata": {},
   "source": [
    "## Model Classes\n",
    "This part defines the model structure. I am referring to the Multi-Head attention approach. I am not going into detail about mathematics. \n",
    "\n",
    "This is the comprised structure:\n",
    "\n",
    "- create **token** and **position embeddings** of the input text and pass them to\n",
    "- a sequential layer stack of `num_blocks` blocks \n",
    "  - pass embeddings to one **MultiHeadAttention** per block consisting of `num_heads` of heads for **self-attention calculation**\n",
    "    - pass result to a linear **projection layer**\n",
    "    - apply dropout of `dropout_rate` nodes \n",
    "  - pass results to the first **normalisation layer**\n",
    "  - pass results to a **feed forward layer**, which consists of a sequential layer stack of\n",
    "    - a linear layer\n",
    "    - a non-linear ReLU layer\n",
    "    - a linear layer\n",
    "    - another dropout layer\n",
    "  - pass results to the second **normalisation layer**\n",
    "- pass results to another **normalisation layer**\n",
    "- pass results to a final **linear layer**\n",
    "- reset the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7c5efd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer_keys = nn.Linear(in_features=embedding_size, out_features=head_size, bias=False)\n",
    "        self.linear_layer_queries = nn.Linear(in_features=embedding_size, out_features=head_size, bias=False)\n",
    "        self.linear_layer_values = nn.Linear(in_features=embedding_size, out_features=head_size, bias=False)\n",
    "        \n",
    "        # lower triangular matrix\n",
    "        # .register_buffer register \"not trainable parameters\" for the model\n",
    "        # .tril() takes a tensor and changes all values above a diagonal (top left to bottom right) to 0\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) \n",
    "\n",
    "        # forget nodes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X_embeddings):\n",
    "        \n",
    "        # input shape (batch_size, sequence_length (time steps), number of features (channels))\n",
    "        # output shape (batch_size, sequence_length (time steps), head_size)\n",
    "            \n",
    "        keys = self.linear_layer_keys(X_embeddings)\n",
    "        queries = self.linear_layer_queries(X_embeddings)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        \n",
    "        # the core of this model: the attention mechanism\n",
    "        # scaled dot product attention by matrix multiplication and scaling the result using square root of the head_size\n",
    "        # we could either divide by square root of head size or multiple by inverse square root of head size, result is the same\n",
    "        scaled_attention_scores = queries @ keys.transpose(-2,-1) * keys.shape[-1] ** -0.5\n",
    "        \n",
    "        # \"sort out\" all the weights above the diagonale to make sure a token depends/attends on/to previous tokens\n",
    "        causal_attention_scores = scaled_attention_scores.masked_fill(self.tril[:block_size, :block_size] == 0, float('-inf'))\n",
    "        \n",
    "        # normalize probabilities\n",
    "        weights = torch.softmax(causal_attention_scores, dim=-1)\n",
    "        \n",
    "        # forget some weights\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        values = self.linear_layer_values(X_embeddings)\n",
    "        weighted_values = weights @ values\n",
    "        return weighted_values\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head() for head in range(num_heads)])\n",
    "        self.projections = nn.Linear(head_size * num_heads, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X_embeddings):\n",
    "        weighted_values = torch.cat([head(X_embeddings) for head in self.heads], dim=-1) \n",
    "        weighted_values = self.dropout(self.projections(weighted_values))\n",
    "        return weighted_values\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features = embedding_size, out_features = hidden_units_factor * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = hidden_units_factor * embedding_size, out_features = embedding_size),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear_layer_stack(X)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention_layer = MultiHeadAttention()\n",
    "        self.normalisation_layer_1 = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "        self.feed_forward_layer = FeedForward(embedding_size)\n",
    "        self.normalisation_layer_2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, X_embeddings):\n",
    "        \n",
    "        y = self.self_attention_layer(X_embeddings)\n",
    "        X_embeddings = self.normalisation_layer_1(X_embeddings + y)\n",
    "        \n",
    "        y = self.feed_forward_layer(X_embeddings)\n",
    "        X_embeddings = self.normalisation_layer_2(X_embeddings + y)\n",
    "        \n",
    "        return X_embeddings\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=embedding_size)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block() for layer in range(num_blocks)])\n",
    "        self.normalisation_layer = nn.LayerNorm(embedding_size)\n",
    "        self.final_layer = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        # X_batch - a batch of blocks (aka sequences), each containing chars\n",
    "\n",
    "        # from a simple encoding like [0] (for `A`) to an embedding vector where each token is represented by it's own vector of size \"embedding_size\" like [0.0265, 0.0334]\n",
    "        X_token_embeddings = self.token_embedding_table(X_batch) \n",
    "        \n",
    "        # block_size - Block Size/Time Steps - how many chars within a block (aka sequence)\n",
    "        X_position_embeddings = self.position_embedding_table(torch.arange(block_size, device = device))\n",
    "        \n",
    "        X_embeddings = X_token_embeddings + X_position_embeddings\n",
    "        \n",
    "        labels = self.blocks(X_embeddings) # (B,T,C)\n",
    "        labels = self.normalisation_layer(labels) # (B,T,C)\n",
    "        labels = self.final_layer(labels) # (Batch Size, Block Size, Vocab Size)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def generate(self, context, max_new_tokens):\n",
    "        # context is encoded prompt as a tensor of floats with shape = (Batch Size, Block Size Dummy) \n",
    "        for new_token in range(max_new_tokens):\n",
    "            # get block from context\n",
    "            block = context[:, -block_size:]\n",
    "            # let the model calculate prediction logits\n",
    "            y_logits = self.forward(block)\n",
    "            # focus only on the last time step\n",
    "            y_logits = y_logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            y_prediction_probs = torch.softmax(y_logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            y_predicted = torch.multinomial(y_prediction_probs, num_samples=1) # (B, 1)\n",
    "            # append predicted label to the sequence\n",
    "            context = torch.cat((context, y_predicted), dim=1) # (B, T+1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b9bec",
   "metadata": {},
   "source": [
    "# Training process\n",
    "## Init the model\n",
    "This is where the preparation ends. We are ready to **init** our model. \n",
    "\n",
    "We are using the **AdamW optimizier** and the common **cross entropy** loss function. \n",
    "\n",
    "If you want to start over, do it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91a3457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = {\n",
    "    'epoch': [],\n",
    "    'loss_train': [],\n",
    "    'loss_test': [],\n",
    "    'duration': 0 # seconds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45d01e",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Now we are ready to start the trainign and test loop. I am referring to the suggested approach by Daniel Bourke (checkout his \"*Optimization loop song*: https://www.youtube.com/watch?v=Nutpusq_AFw).\n",
    "\n",
    "This is the rough architecture behind the loop:\n",
    "\n",
    "- get a fresh and random batch of data\n",
    "- pass training data (`features_train`) to the model\n",
    "- calculate the loss by comparing the predicted results `labels_logits`to the expected results `labels_traing`\n",
    "- zero grad the optimizer\n",
    "- do back propagation of the loss\n",
    "- update parameters via optimizer function\n",
    "- start testing in torch's inference mode\n",
    "- calclate the test los by comparing the predicted test results `labels_logits` to the expected test results `labels_test`\n",
    "\n",
    "We will record train and test losses to plot them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b3838bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Batch Size: 8\n",
      "Block Size: 8\n",
      "Epochs: 1000\n",
      "Learning Rate: 0.0001\n",
      "Embeddings: 128\\Blocks: 4\n",
      "Heads: 4\n",
      "Dropout Rate: 0.2\n",
      "Hidden Units Factor: 4\n",
      "Train/Test Split: 0.8\n",
      "--\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0152,  0.0101,  0.0005,  ...,  0.0024, -0.0027, -0.0063],\n",
      "        [-0.0203,  0.0003, -0.0061,  ..., -0.0117, -0.0171,  0.0126],\n",
      "        [ 0.0157,  0.0207,  0.0206,  ...,  0.0077,  0.0059, -0.0419],\n",
      "        ...,\n",
      "        [ 0.0335,  0.0315, -0.0021,  ...,  0.0176,  0.0284,  0.0229],\n",
      "        [ 0.0066, -0.0229,  0.0170,  ...,  0.0095,  0.0187, -0.0057],\n",
      "        [-0.0014,  0.0187, -0.0018,  ...,  0.0013,  0.0130,  0.0494]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0152,  0.0101,  0.0005,  ...,  0.0024, -0.0027, -0.0063],\n",
      "        [-0.0203,  0.0003, -0.0061,  ..., -0.0117, -0.0171,  0.0126],\n",
      "        [ 0.0157,  0.0207,  0.0206,  ...,  0.0077,  0.0059, -0.0419],\n",
      "        ...,\n",
      "        [ 0.0335,  0.0315, -0.0021,  ...,  0.0176,  0.0284,  0.0229],\n",
      "        [ 0.0066, -0.0229,  0.0170,  ...,  0.0095,  0.0187, -0.0057],\n",
      "        [-0.0014,  0.0187, -0.0018,  ...,  0.0013,  0.0130,  0.0494]],\n",
      "       device='mps:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# PREDICTIONS\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m labels_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# LOSS\u001b[39;00m\n\u001b[1;32m     21\u001b[0m curr_batches, curr_blocks, curr_classes \u001b[38;5;241m=\u001b[39m labels_logits\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 130\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[0;34m(self, X_encoded)\u001b[0m\n\u001b[1;32m    127\u001b[0m X_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device \u001b[38;5;241m=\u001b[39m device)) \u001b[38;5;66;03m# (T,C) Ã¤ same, but now for the position, which leads to different vectors\u001b[39;00m\n\u001b[1;32m    128\u001b[0m X_embeddings \u001b[38;5;241m=\u001b[39m X_token_embeddings \u001b[38;5;241m+\u001b[39m X_position_embeddings \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_embeddings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalisation_layer(labels) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(labels) \u001b[38;5;66;03m# (Batch Size, Block Size, Vocab Size)\u001b[39;00m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 87\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, X_embeddings)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_embeddings):\n\u001b[0;32m---> 87\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     X_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalisation_layer_1(X_embeddings \u001b[38;5;241m+\u001b[39m y)\n\u001b[1;32m     90\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_layer(X_embeddings)\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 54\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, X_embeddings)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_embeddings):\n\u001b[0;32m---> 54\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([head(X_embeddings) \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojections(y))\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "Cell \u001b[0;32mIn[35], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_embeddings):\n\u001b[0;32m---> 54\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_embeddings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojections(y))\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development.private/python/modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, X_embeddings)\u001b[0m\n\u001b[1;32m     24\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys(X_embeddings)   \u001b[38;5;66;03m# (B,T,head_size)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 26\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39mexit()\n\u001b[1;32m     28\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries(X_embeddings) \u001b[38;5;66;03m# (B,T,head_size)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# compute attention scores (\"affinities\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Matrix multiplication and scaling the result using square root of the head_size\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {device}\\nBatch Size: {batch_size}\\nBlock Size: {block_size}\\nEpochs: {epochs}\\nLearning Rate: {learning_rate}\\n\"\n",
    "      f\"Embeddings: {embedding_size}\\Blocks: {num_blocks}\\nHeads: {num_heads}\\n\"\n",
    "      f\"Dropout Rate: {dropout_rate}\\nHidden Units Factor: {hidden_units_factor}\\nTrain/Test Split: {train_split}\\n--\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "start_epoch = 0 if len(losses['epoch']) == 0 else losses['epoch'][-1]\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "\n",
    "    # GET BATCH OF TRAINING DATA\n",
    "    # ALREADY CONTAINS CHAR-ENCODED TEXT LIKE `3, 6, 7, 7, 8` for `HELLO`\n",
    "    X_train, y_train = get_batch('train')\n",
    "    # PREDICTIONS\n",
    "    labels_logits = model(X_train)\n",
    "\n",
    "    # LOSS\n",
    "    curr_batches, curr_blocks, curr_classes = labels_logits.shape\n",
    "    labels_logits = labels_logits.view(curr_batches * curr_blocks, curr_classes) # merge Batches to one single batch, also keep number of classes\n",
    "    y_train = y_train.view(curr_batches * curr_blocks) # merge Batches to one single batch, no classes required for cross_entropy\n",
    "    loss_train = loss_fn(labels_logits, y_train)\n",
    "\n",
    "    # SET OPTIMIZER TO ZERO\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # BACK PROPAGATION \n",
    "    loss_train.backward()\n",
    "\n",
    "    # UPDATE PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    # LOG\n",
    "    if epochs < 10 or epoch % int(epochs / 10) == 0:\n",
    "\n",
    "      # TEST  \n",
    "      model.eval()\n",
    "      with torch.inference_mode():\n",
    "        features_test, labels_test = get_batch('test')\n",
    "        labels_logits = model(features_test)\n",
    "        curr_batches, curr_blocks, curr_classes = labels_logits.shape\n",
    "        labels_logits = labels_logits.view(curr_batches * curr_blocks, curr_classes) # merge Batches to one single batch, also keep number of classes\n",
    "        labels_test = labels_test.view(curr_batches * curr_blocks) # merge Batches to one single batch, no classes required for cross_entropy\n",
    "        loss_test = loss_fn(labels_logits, labels_test)\n",
    "\n",
    "      print(f\"Epoch {epoch} | Train loss {loss_train:.2f} | Test Loss {loss_test:.2f}\")\n",
    "      \n",
    "      losses['epoch'].append(epoch)\n",
    "      losses['loss_train'].append(loss_train.cpu().item())\n",
    "      losses['loss_test'].append(loss_test.cpu().item())\n",
    "      losses['duration'] += (time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18076e3f",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "We will plot train and test losses for each epoch to see how the loss goes down - hopefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae508674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "fontsize = 10\n",
    "line_width = 0.5\n",
    "y_axis_limit = int(max(losses['loss_train'] + losses['loss_test']) * 1.25)\n",
    "\n",
    "ax1.plot(losses['epoch'], losses['loss_train'], color='red', label='Train Loss', linewidth=line_width + 1)\n",
    "ax1.set_xlabel('Epoch', fontsize=fontsize)\n",
    "ax1.set_ylabel('Train Loss', color='red', fontsize=fontsize)\n",
    "ax1.tick_params('y', colors='red', labelsize=fontsize)\n",
    "ax1.set_ylim(0, y_axis_limit) \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(losses['epoch'], losses['loss_test'], color='green', label='Test Loss', linewidth=line_width)\n",
    "ax2.set_ylabel('Test Loss', color='green', fontsize=fontsize)\n",
    "ax2.tick_params('y', colors='green', labelsize=fontsize)\n",
    "ax2.set_ylim(0, y_axis_limit) \n",
    "\n",
    "plt.title(f'Train/Test Loss after {(losses[\"duration\"]):.2f} seconds')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568a1fe",
   "metadata": {},
   "source": [
    "# Aftermath\n",
    "## Loading and saving the model\n",
    "\n",
    "We can utilize two functions to persist the model's result\n",
    "- either putting everything to a pickel file or\n",
    "- using PyTorch's state dic export feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb122f",
   "metadata": {},
   "source": [
    "torch.save(obj = model.state_dict(), f = model_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "779577f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_export))\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'vocab_size'"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load(model_export))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc34d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model-01.pkl', 'rb') as f:\n",
    "     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4001618",
   "metadata": {},
   "source": [
    "# Use the model\n",
    "Finally, apparently, we want to use the model. This simply generates new tokens for a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb9870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World! Hllo World! Hello \n",
      "Hello World! Hello Wold! \n",
      "Hello\n",
      "Hello World! \n",
      "Hello World! \n",
      "Hel\n",
      "Hello World! Hello World! \n",
      "Hell\n",
      "Hello World! Hel\n",
      "Hello World! H\n",
      "Hellorld! eld! Hello World! \n",
      "He\n",
      "Hello World! \n",
      "Hello World! \n",
      "Hel\n",
      "Hello World! \n",
      "Hello World! ! He\n",
      "Hello World! Hello World! \n",
      "Hell\n",
      "Hello Wrld! Hello World! Hello \n"
     ]
    }
   ],
   "source": [
    "prompt = 'H'\n",
    "context = torch.tensor(encode(char_table, prompt), dtype=torch.long, device=device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for iter in range(10):    \n",
    "    generated_chars = decode(reverse_char_table, model.generate(context.unsqueeze(0), max_new_tokens = 30)[0].tolist())\n",
    "    print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5a721",
   "metadata": {},
   "source": [
    "# Step By Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16b56a",
   "metadata": {},
   "source": [
    "We will use a simple *text* containing 100 times the same chars: *ABCA*. First we need to encode it, simply using an integer char table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "10a7503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 chars in our dataset: ABCAABCAABCAABCAABCA\n",
      "Char table: {'A': 0, 'B': 1, 'C': 2}\n",
      "Vocabulary size is: 3\n"
     ]
    }
   ],
   "source": [
    "data = 'ABCA' * 100\n",
    "\n",
    "chars = sorted(list(set(''.join(data))))\n",
    "vocab_size = len(chars)\n",
    "char_table = {char:index for index,char in enumerate(chars)}\n",
    "encoded_data = torch.tensor(encode(char_table = char_table, text = data), dtype=torch.long)\n",
    "\n",
    "print(f'First 20 chars in our dataset: {data[:20]}')\n",
    "print(f'Char table: {char_table}')\n",
    "print(f'Vocabulary size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd683a",
   "metadata": {},
   "source": [
    "Now we will take ```batch_size``` sequences of length ```block_size``` from random positions in text, in this case **2 blocks** with a length of **4 chars** each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b99b408c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 0],\n",
       "        [0, 0, 1, 2]], device='mps:0')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 4 # tokens per block/sequence\n",
    "batch_size = 2 # blocks per batch\n",
    "\n",
    "random_block_start_indices = torch.randint(len(encoded_data) - block_size, (batch_size,))\n",
    "            \n",
    "X_batch = torch.stack([encoded_data[index:index + block_size] for index in random_block_start_indices])\n",
    "X_batch = X_batch.to(device)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6977a0",
   "metadata": {},
   "source": [
    "This gives us a batch of blocks (or sequences) that we can use for training. \n",
    "We could translate this back to readable chars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "962b5bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABCA', 'AABC']\n"
     ]
    }
   ],
   "source": [
    "print([''.join(list(char_table.keys())[idx] for idx in sequence) for sequence in X_batch.cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a622340f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7042,  0.7347,  0.1688],\n",
       "         [ 0.1699, -0.4541, -1.7925],\n",
       "         [-0.1837, -0.4113, -1.7933],\n",
       "         [ 1.7042,  0.7347,  0.1688]],\n",
       "\n",
       "        [[ 1.7042,  0.7347,  0.1688],\n",
       "         [ 1.7042,  0.7347,  0.1688],\n",
       "         [ 0.1699, -0.4541, -1.7925],\n",
       "         [-0.1837, -0.4113, -1.7933]]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 3\n",
    "\n",
    "token_embeddings_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size).to(device)\n",
    "X_token_embeddings = token_embeddings_table(X_batch)\n",
    "X_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "133176bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3], device='mps:0')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size).to(device)\n",
    "positions = torch.arange(block_size, device = device)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "bf967c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8644, -0.6244,  1.1936],\n",
       "        [ 1.8613,  1.4914, -1.2505],\n",
       "        [-0.3755, -0.2110,  0.7012],\n",
       "        [ 0.0000,  0.0000,  0.0000]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_position_embeddings = position_embeddings_table(positions)\n",
    "X_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "860bb5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8821,  1.9137,  1.6605],\n",
       "         [ 0.1674,  0.3899, -1.0054],\n",
       "         [-0.7229,  0.5058, -3.5047],\n",
       "         [ 1.7042,  0.7347,  0.1688]],\n",
       "\n",
       "        [[ 0.8821,  1.9137,  1.6605],\n",
       "         [ 1.7017,  1.5787,  0.9559],\n",
       "         [-0.3693,  0.4630, -3.5038],\n",
       "         [-0.1837, -0.4113, -1.7933]]], device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings = X_token_embeddings + X_position_embeddings\n",
    "X_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0559b17",
   "metadata": {},
   "source": [
    "Now we'll send our embeddings through the Block-Class and a the MultiHeadAttention-Class to the Head-Class where we first calculate our ```keys``` and ```queries``` using a *Linear Layer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a7aaee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5929],\n",
      "         [ 0.5622],\n",
      "         [ 1.9725],\n",
      "         [-0.1752]],\n",
      "\n",
      "        [[-0.5929],\n",
      "         [-0.4113],\n",
      "         [ 1.9159],\n",
      "         [ 0.8550]]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([[[-0.1706],\n",
      "         [-0.2536],\n",
      "         [-0.5840],\n",
      "         [-0.2957]],\n",
      "\n",
      "        [[-0.1706],\n",
      "         [-0.3180],\n",
      "         [-0.6159],\n",
      "         [-0.1994]]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# this is for training purposes, if we had multiple heads, we'd set this to embedding_size / num_heads, now every token is represented by\n",
    "# one scalar value only, which does not really make sense\n",
    "head_size = 1 \n",
    "\n",
    "linear_layer_keys = nn.Linear(in_features=embedding_size, out_features=head_size, bias=False).to(device)\n",
    "linear_layer_queries = nn.Linear(in_features=embedding_size, out_features=head_size, bias=False).to(device)\n",
    "keys = linear_layer_keys(X_embeddings)\n",
    "queries = linear_layer_queries(X_embeddings)\n",
    "print(keys)\n",
    "print(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7123bfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1012, -0.0959, -0.3366,  0.0299],\n",
       "         [ 0.1504, -0.1426, -0.5002,  0.0444],\n",
       "         [ 0.3463, -0.3283, -1.1520,  0.1023],\n",
       "         [ 0.1753, -0.1662, -0.5833,  0.0518]],\n",
       "\n",
       "        [[ 0.1012,  0.0702, -0.3269, -0.1459],\n",
       "         [ 0.1886,  0.1308, -0.6093, -0.2719],\n",
       "         [ 0.3652,  0.2533, -1.1800, -0.5266],\n",
       "         [ 0.1182,  0.0820, -0.3821, -0.1705]]], device='mps:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_attention_scores = queries @ keys.transpose(-2,-1) * keys.shape[-1] ** -0.5\n",
    "scaled_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5df3ebfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]], device='mps:0')"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"sort out\" all the weights above the diagonale to make sure a token depends/attends on/to previous tokens\n",
    "triangular_matrix= torch.tril(torch.ones(block_size, block_size)).to(device)\n",
    "triangular_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9473a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1012,    -inf,    -inf,    -inf],\n",
       "         [ 0.1504, -0.1426,    -inf,    -inf],\n",
       "         [ 0.3463, -0.3283, -1.1520,    -inf],\n",
       "         [ 0.1753, -0.1662, -0.5833,  0.0518]],\n",
       "\n",
       "        [[ 0.1012,    -inf,    -inf,    -inf],\n",
       "         [ 0.1886,  0.1308,    -inf,    -inf],\n",
       "         [ 0.3652,  0.2533, -1.1800,    -inf],\n",
       "         [ 0.1182,  0.0820, -0.3821, -0.1705]]], device='mps:0',\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_scores = scaled_attention_scores.masked_fill(triangular_matrix[:block_size, :block_size] == 0, float('-inf'))\n",
    "causal_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "053e7d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5727, 0.4273, 0.0000, 0.0000],\n",
       "         [0.5771, 0.2939, 0.1290, 0.0000],\n",
       "         [0.3265, 0.2320, 0.1529, 0.2886]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5144, 0.4856, 0.0000, 0.0000],\n",
       "         [0.4745, 0.4243, 0.1012, 0.0000],\n",
       "         [0.3012, 0.2905, 0.1826, 0.2257]]], device='mps:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize probabilities\n",
    "weights = torch.softmax(causal_attention_scores, dim=-1) # (B, T, T)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7e3e488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7159, 0.5341, 0.0000, 0.0000],\n",
       "         [0.7213, 0.3674, 0.1612, 0.0000],\n",
       "         [0.4081, 0.2900, 0.1911, 0.3607]],\n",
       "\n",
       "        [[1.2500, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.6070, 0.0000, 0.0000],\n",
       "         [0.5931, 0.5304, 0.0000, 0.0000],\n",
       "         [0.3765, 0.3631, 0.2283, 0.2821]]], device='mps:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.2 # how many nodes to drop per each epoch\n",
    "\n",
    "# forget some weights\n",
    "dropout = nn.Dropout(dropout_rate)\n",
    "weights = dropout(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a1127738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000],\n",
       "         [ 0.1185],\n",
       "         [ 0.0589],\n",
       "         [-0.2447]],\n",
       "\n",
       "        [[ 0.5004],\n",
       "         [-0.0981],\n",
       "         [ 0.1517],\n",
       "         [-0.2473]]], device='mps:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_values = nn.Linear(in_features=embedding_size, out_features=head_size, bias=False).to(device)\n",
    "values = linear_layer_values(X_embeddings)\n",
    "weighted_values = weights @ values\n",
    "weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffd82f",
   "metadata": {},
   "source": [
    "In the next step, within the MultiHeadAttention-Class, the weighted values of each head will now be concatenated and another dropout will be applied. We can skip this, as we only simulate one head here. The final result will be send back to the Block-Class, where it will be processed further. First step is a Normalisation Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6580e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation_layer_1 = nn.LayerNorm(weighted_values)\n",
    "X_embeddings = normalisation_layer_1(X_embeddings + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_layer = FeedForward(embedding_size)\n",
    "self.normalisation_layer_2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "def forward(self, X_embeddings):\n",
    "\n",
    "y = self.self_attention_layer(X_embeddings)\n",
    "y = self.feed_forward_layer(X_embeddings)\n",
    "X_embeddings = self.normalisation_layer_2(X_embeddings + y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
