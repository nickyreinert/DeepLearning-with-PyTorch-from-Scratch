{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229eb15f",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook has to main parts. The first part will guide you through the \"compact\" build process of the model and it's surroundings. The second part will go through the calculation process step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df03f7b",
   "metadata": {},
   "source": [
    "# Part 1 - Built the Model and Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fadf3",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84aa063",
   "metadata": {},
   "source": [
    "### Optional: GDrive connection\n",
    "If you want to run this on Google Colab, you can connect the notebook to your Google Drive to access training data from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3bea7a",
   "metadata": {},
   "source": [
    "### Libraries and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c351b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on `mps`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mmap\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "source_data_path = \"./\"\n",
    "source_data = \"text.txt\"\n",
    "model_export = \"model.pth\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else device\n",
    "\n",
    "print(f'Running on `{device}`')\n",
    "\n",
    "batch_size = 8 # how many blocks per batch\n",
    "sequence_length = 8 # length of sequence of tokens, how many chars/words per sequence \n",
    "epochs = 10 # training iterations\n",
    "learning_rate = 0.0001 # step size per epoch\n",
    "embedding_size = 128 # the size of the embedding vectors that each token will be assigned to, like if set to 128, one token would be embedded into a vector of size 128\n",
    "num_transformer_blocks = 4 # amount of transformer blocks\n",
    "num_heads = 4 # amount of attention heads (per block), should be bigger as embedding_size, see next line\n",
    "attention_head_size = embedding_size // num_heads # dont change this, defining it here for better readability, using this operation simplifies process\n",
    "dropout_rate = 0.2 # how many nodes to drop per each epoch\n",
    "hidden_units_factor = 4 # how many hidden units for the sequential layer stack inside Feedforward-Module, using a factor here, will be multiplied with embedding_size, increases the models capability to process more complex patterns, also increases computational power\n",
    "train_split = 0.8 # test and train dataset split rate\n",
    "\n",
    "losses = {} # for statistical reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b370ca8",
   "metadata": {},
   "source": [
    "## Data\n",
    "I am giving you two options here: Either create a dummy data set with a lot of simple repetitions. This allows the model to learn fast. Or choose a real world example and skip the \"Dummy Data part\". \n",
    "\n",
    "If you are looking for better results, you may refer to e.g. **OpenWebText** or other available data sets. \n",
    "\n",
    "Also please note, that this basic implementation uses chars as token. If you want to tokenize on a word-base, you may implement a more sophisticated tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393be78c",
   "metadata": {},
   "source": [
    "#### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e9f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello World! Hello World! Hello World! \\n\"\n",
    "f_dummy_data = open(f\"{source_data_path}/{source_data}\", 'w')\n",
    "f_dummy_data.write(sentence * 1000)\n",
    "f_dummy_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ab082",
   "metadata": {},
   "source": [
    "#### Prepare\n",
    "We are going to read the data (which can be found in ```source_data``` as defined on top) and split it into two parts: **Training** data and **testing** data. \n",
    "\n",
    "Next we get a list of all used `chars` and create two tables for encoding and decoding(`char_table` and `reverse_char_table`) text into vector of integers. The `vocab_size` shows the amount of different chars in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516b1cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 10\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "full_text = []\n",
    "source_data_train = f\"train_{source_data}\"\n",
    "source_data_test = f\"test_{source_data}\"\n",
    "\n",
    "f_full_text = open(f\"{source_data_path}/{source_data}\", \"r\")\n",
    "f_training_text = open(f\"{source_data_path}/{source_data_train}\", 'w')\n",
    "f_test_test = open(f\"{source_data_path}/{source_data_test}\", 'w')\n",
    "\n",
    "for line in f_full_text:\n",
    "  r = random.random()\n",
    "  full_text.append(line)\n",
    "  if (0.0 <=  r <= train_split):\n",
    "    f_training_text.write(line)\n",
    "  else:\n",
    "    f_test_test.write(line)\n",
    "\n",
    "f_full_text.close()\n",
    "f_training_text.close()\n",
    "f_test_test.close()\n",
    "\n",
    "chars = sorted(list(set(' '.join(full_text))))\n",
    "char_table = {char:index for index,char in enumerate(chars)}\n",
    "reverse_char_table = {index:char for index,char in enumerate(char_table)}\n",
    "\n",
    "vocab_size = len(chars)\n",
    "del full_text\n",
    "\n",
    "print(f'Vocabulary size is {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205869ed",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3a686",
   "metadata": {},
   "source": [
    "We will create four functions:\n",
    "`encode()` and `decode()` will allow us to convert text to a vector of integers and back. \n",
    "\n",
    "`get_batch` and `get_random_chunk` will randomly read text chunks from our **training** and **testing** data set.\n",
    "\n",
    "`get_random_chunk` returns one big block of from a random position in the source file.\n",
    "\n",
    "`get_batch` will dismantle the block into smaller sequences with a length of `sequence_length`. It will split the data into train and test data sets. The test data set is shifted by one char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506ad003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(char_table: list, text: str = None):\n",
    "  return [char_table[char] for char in text]\n",
    "\n",
    "def decode(char_table: list, numbers: list = []):\n",
    "  return ''.join([char_table[index] for index in numbers])\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = f\"{source_data_path}/{source_data_train}\" if split == 'train' else f\"{source_data_path}/{source_data_test}\"\n",
    "    with open(filename, 'r') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - sequence_length * batch_size)\n",
    "\n",
    "            # Seek to the random position and read data binary\n",
    "            mm.seek(start_pos)\n",
    "            raw_data = mm.read(sequence_length * batch_size - 1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            data = raw_data.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            # encode to torch tensor using char table\n",
    "            encoded_data = torch.tensor(encode(char_table = char_table, text = data), dtype=torch.long)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    encoded_data = get_random_chunk(split)\n",
    "    random_block_start_indices = torch.randint(len(encoded_data) - sequence_length, (batch_size,))\n",
    "\n",
    "    input_sequences = torch.stack([encoded_data[index:index + sequence_length] for index in random_block_start_indices])\n",
    "    target_sequences = torch.stack([encoded_data[index + 1:index + sequence_length + 1] for index in random_block_start_indices])\n",
    "\n",
    "    return input_sequences.to(device), target_sequences.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279da1e",
   "metadata": {},
   "source": [
    "The following part just tests encoding and decoding feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33eded27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding `Hello World! ` to  `[3, 6, 7, 7, 8, 1, 4, 8, 9, 7, 5, 2] ` and back to  `Hello World! `\n"
     ]
    }
   ],
   "source": [
    "text = 'Hello World!'\n",
    "encoded_text = encode(char_table, 'Hello World!')\n",
    "decoded_text = decode(reverse_char_table, encoded_text)\n",
    "print(f'Encoding `{text} ` to  `{encoded_text} ` and back to  `{decoded_text} `')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643755c6",
   "metadata": {},
   "source": [
    "## Model Classes\n",
    "This part defines the model structure. I am referring to the Multi-Head attention approach. I am not going into detail about mathematics. \n",
    "\n",
    "This is the comprised structure:\n",
    "\n",
    "- create **token** and **position embeddings** of the input text and pass them to\n",
    "- a sequential layer stack of `num_blocks` blocks \n",
    "  - pass embeddings to one **MultiHeadAttention** per block consisting of `num_heads` of heads for **self-attention calculation**\n",
    "    - pass result to a linear **projection layer**\n",
    "    - apply dropout of `dropout_rate` nodes \n",
    "  - pass results to the first **normalisation layer**\n",
    "  - pass results to a **feed forward layer**, which consists of a sequential layer stack of\n",
    "    - a linear layer\n",
    "    - a non-linear ReLU layer\n",
    "    - a linear layer\n",
    "    - another dropout layer\n",
    "  - pass results to the second **normalisation layer**\n",
    "- pass results to another **normalisation layer**\n",
    "- pass results to a final **linear layer**\n",
    "- reset the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c5efd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layer_keys = nn.Linear(in_features=embedding_size, out_features=attention_head_size, bias=False)\n",
    "        self.linear_layer_queries = nn.Linear(in_features=embedding_size, out_features=attention_head_size, bias=False)\n",
    "        self.linear_layer_values = nn.Linear(in_features=embedding_size, out_features=attention_head_size, bias=False)\n",
    "        \n",
    "        # lower triangular matrix\n",
    "        # .register_buffer register \"not trainable parameters\" for the model\n",
    "        # .tril() takes a tensor and changes all values above a diagonal (top left to bottom right) to 0\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(sequence_length, sequence_length))) \n",
    "\n",
    "        # forget nodes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X_embeddings):\n",
    "        \n",
    "        # input shape (batch_size, sequence_length (time steps), number of features (channels))\n",
    "        # output shape (batch_size, sequence_length (time steps), head_size)\n",
    "\n",
    "        keys = self.linear_layer_keys(X_embeddings)\n",
    "        queries = self.linear_layer_queries(X_embeddings)\n",
    "        values = self.linear_layer_values(X_embeddings)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        \n",
    "        # the core of this model: the attention mechanism\n",
    "        # scaled dot product attention by matrix multiplication and scaling the result using square root of the head_size\n",
    "        # we could either divide by square root of head size or multiple by inverse square root of head size, result is the same\n",
    "        scaled_attention_scores = queries @ keys.transpose(-2,-1) * keys.shape[-1] ** -0.5\n",
    "        \n",
    "        # \"sort out\" all the weights above the diagonale to make sure a token depends/attends on/to previous tokens\n",
    "        causal_attention_scores = scaled_attention_scores.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        \n",
    "        # normalize probabilities\n",
    "        weights = torch.softmax(causal_attention_scores, dim=-1)\n",
    "        \n",
    "        # forget some weights\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        weighted_values = weights @ values\n",
    "        return weighted_values\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel, in contrast to the \"classical\" encoder-decoder attention \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head() for head in range(num_heads)])\n",
    "        self.projections = nn.Linear(attention_head_size * num_heads, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X_embeddings):\n",
    "        weighted_values = torch.cat([head(X_embeddings) for head in self.heads], dim=-1) \n",
    "        weighted_values = self.dropout(self.projections(weighted_values))\n",
    "        return weighted_values\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features = embedding_size, out_features = hidden_units_factor * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = hidden_units_factor * embedding_size, out_features = embedding_size),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear_layer_stack(X)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention_layer = MultiHeadAttention()\n",
    "        self.layer_normalisation_1 = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "        self.feed_forward_layer = FeedForward()\n",
    "        self.layer_normalisation_2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, X_embeddings):\n",
    "        \n",
    "        y = self.self_attention_layer(X_embeddings) # relationships and interactions, create a context\n",
    "        X_embeddings = self.layer_normalisation_1(X_embeddings + y) # residual connection to mitigate the vanishing gradient\n",
    "        \n",
    "        y = self.feed_forward_layer(X_embeddings) # refine individual characteristics\n",
    "        X_embeddings = self.layer_normalisation_2(X_embeddings + y) # residual connection to mitigate the vanishing gradient\n",
    "        \n",
    "        return X_embeddings\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=sequence_length, embedding_dim=embedding_size)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[TransformerBlock() for layer in range(num_transformer_blocks)])\n",
    "        self.layer_normalisation = nn.LayerNorm(embedding_size)\n",
    "        self.final_layer = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        # X_batch - a batch of blocks (aka sequences), each containing chars\n",
    "\n",
    "        # from a simple encoding like [0] (for `A`) to an embedding vector where each token is represented by it's own vector of size \"embedding_size\" like [0.0265, 0.0334]\n",
    "        X_token_embeddings = self.token_embedding_table(X_batch) \n",
    "        \n",
    "        # sequence_length - Block Size/Time Steps - how many chars within a block (aka sequence)\n",
    "        X_position_embeddings = self.position_embedding_table(torch.arange(sequence_length, device = device))\n",
    "        \n",
    "        X_embeddings = X_token_embeddings + X_position_embeddings\n",
    "        \n",
    "        labels = self.blocks(X_embeddings) # (B,T,C)\n",
    "        labels = self.layer_normalisation(labels) # (B,T,C)\n",
    "        labels = self.final_layer(labels) # (Batch Size, Block Size, Vocab Size)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def generate(self, context, max_new_tokens):\n",
    "        # context is encoded prompt as a tensor of floats with shape = (Batch Size, Block Size Dummy) \n",
    "        for new_token in range(max_new_tokens):\n",
    "            # get block from context\n",
    "            block = context[:, -sequence_length:]\n",
    "            # let the model calculate prediction logits\n",
    "            y_logits = self.forward(block)\n",
    "            # focus only on the last time step\n",
    "            y_logits = y_logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            y_prediction_probs = torch.softmax(y_logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            y_predicted = torch.multinomial(y_prediction_probs, num_samples=1) # (B, 1)\n",
    "            # append predicted label to the sequence\n",
    "            context = torch.cat((context, y_predicted), dim=1) # (B, T+1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b9bec",
   "metadata": {},
   "source": [
    "## Training process\n",
    "### Init the model\n",
    "This is where the preparation ends. We are ready to **init** our model. \n",
    "\n",
    "We are using the **AdamW optimizier** and the common **cross entropy** loss function. \n",
    "\n",
    "If you want to start over, do it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91a3457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = {\n",
    "    'epoch': [],\n",
    "    'loss_train': [],\n",
    "    'loss_test': [],\n",
    "    'duration': 0 # seconds\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45d01e",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "Now we are ready to start the trainign and test loop. I am referring to the suggested approach by Daniel Bourke (checkout his \"*Optimization loop song*: https://www.youtube.com/watch?v=Nutpusq_AFw).\n",
    "\n",
    "This is the rough architecture behind the loop:\n",
    "\n",
    "- get a fresh and random batch of data\n",
    "- pass training data (`features_train`) to the model\n",
    "- calculate the loss by comparing the predicted results `labels_logits`to the expected results `labels_traing`\n",
    "- zero grad the optimizer\n",
    "- do back propagation of the loss\n",
    "- update parameters via optimizer function\n",
    "- start testing in torch's inference mode\n",
    "- calclate the test los by comparing the predicted test results `labels_logits` to the expected test results `labels_test`\n",
    "\n",
    "We will record train and test losses to plot them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b3838bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Batch Size: 8\n",
      "Block Size: 8\n",
      "Epochs: 10\n",
      "Learning Rate: 0.0001\n",
      "Embeddings: 128\\Blocks: 4\n",
      "Heads: 4\n",
      "Dropout Rate: 0.2\n",
      "Hidden Units Factor: 4\n",
      "Train/Test Split: 0.8\n",
      "--\n",
      "\n",
      "Epoch 0 | Train loss 2.39 | Test Loss 2.29\n",
      "Epoch 1 | Train loss 2.35 | Test Loss 2.32\n",
      "Epoch 2 | Train loss 2.28 | Test Loss 2.27\n",
      "Epoch 3 | Train loss 2.28 | Test Loss 2.26\n",
      "Epoch 4 | Train loss 2.24 | Test Loss 2.23\n",
      "Epoch 5 | Train loss 2.22 | Test Loss 2.18\n",
      "Epoch 6 | Train loss 2.19 | Test Loss 2.17\n",
      "Epoch 7 | Train loss 2.21 | Test Loss 2.17\n",
      "Epoch 8 | Train loss 2.16 | Test Loss 2.10\n",
      "Epoch 9 | Train loss 2.11 | Test Loss 2.10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {device}\\nBatch Size: {batch_size}\\nBlock Size: {sequence_length}\\nEpochs: {epochs}\\nLearning Rate: {learning_rate}\\n\"\n",
    "      f\"Embeddings: {embedding_size}\\Blocks: {num_transformer_blocks}\\nHeads: {num_heads}\\n\"\n",
    "      f\"Dropout Rate: {dropout_rate}\\nHidden Units Factor: {hidden_units_factor}\\nTrain/Test Split: {train_split}\\n--\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "start_epoch = 0 if len(losses['epoch']) == 0 else losses['epoch'][-1]\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "\n",
    "    # GET BATCH OF TRAINING DATA\n",
    "    # ALREADY CONTAINS CHAR-ENCODED TEXT LIKE `3, 6, 7, 7, 8` for `HELLO`\n",
    "    X_train, y_train = get_batch('train')\n",
    "    # PREDICTIONS\n",
    "    labels_logits = model(X_train)\n",
    "\n",
    "    # LOSS\n",
    "    curr_batches, curr_blocks, curr_classes = labels_logits.shape\n",
    "    labels_logits = labels_logits.view(curr_batches * curr_blocks, curr_classes) # merge Batches to one single batch, also keep number of classes\n",
    "    y_train = y_train.view(curr_batches * curr_blocks) # merge Batches to one single batch, no classes required for cross_entropy\n",
    "    loss_train = loss_fn(labels_logits, y_train)\n",
    "\n",
    "    # SET OPTIMIZER TO ZERO\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # BACK PROPAGATION \n",
    "    loss_train.backward()\n",
    "\n",
    "    # UPDATE PARAMETERS\n",
    "    optimizer.step()\n",
    "\n",
    "    # LOG\n",
    "    if epochs < 10 or epoch % int(epochs / 10) == 0:\n",
    "\n",
    "      # TEST  \n",
    "      model.eval()\n",
    "      with torch.inference_mode():\n",
    "        features_test, labels_test = get_batch('test')\n",
    "        labels_logits = model(features_test)\n",
    "        curr_batches, curr_blocks, curr_classes = labels_logits.shape\n",
    "        labels_logits = labels_logits.view(curr_batches * curr_blocks, curr_classes) # merge Batches to one single batch, also keep number of classes\n",
    "        labels_test = labels_test.view(curr_batches * curr_blocks) # merge Batches to one single batch, no classes required for cross_entropy\n",
    "        loss_test = loss_fn(labels_logits, labels_test)\n",
    "\n",
    "      print(f\"Epoch {epoch} | Train loss {loss_train:.2f} | Test Loss {loss_test:.2f}\")\n",
    "      \n",
    "      losses['epoch'].append(epoch)\n",
    "      losses['loss_train'].append(loss_train.cpu().item())\n",
    "      losses['loss_test'].append(loss_test.cpu().item())\n",
    "      losses['duration'] += (time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18076e3f",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "We will plot train and test losses for each epoch to see how the loss goes down - hopefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae508674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(5, 5))\n",
    "fontsize = 10\n",
    "line_width = 0.5\n",
    "y_axis_limit = int(max(losses['loss_train'] + losses['loss_test']) * 1.25)\n",
    "\n",
    "ax1.plot(losses['epoch'], losses['loss_train'], color='red', label='Train Loss', linewidth=line_width + 1)\n",
    "ax1.set_xlabel('Epoch', fontsize=fontsize)\n",
    "ax1.set_ylabel('Train Loss', color='red', fontsize=fontsize)\n",
    "ax1.tick_params('y', colors='red', labelsize=fontsize)\n",
    "ax1.set_ylim(0, y_axis_limit) \n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(losses['epoch'], losses['loss_test'], color='green', label='Test Loss', linewidth=line_width)\n",
    "ax2.set_ylabel('Test Loss', color='green', fontsize=fontsize)\n",
    "ax2.tick_params('y', colors='green', labelsize=fontsize)\n",
    "ax2.set_ylim(0, y_axis_limit) \n",
    "\n",
    "plt.title(f'Train/Test Loss after {(losses[\"duration\"]):.2f} seconds')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568a1fe",
   "metadata": {},
   "source": [
    "## Aftermath\n",
    "### Loading and saving the model\n",
    "\n",
    "We can utilize two functions to persist the model's result\n",
    "- either putting everything to a pickel file or\n",
    "- using PyTorch's state dic export feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bc413b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj = model.state_dict(), f = model_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "779577f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(10, 128)\n",
       "  (position_embedding_table): Embedding(8, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (self_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (linear_layer_keys): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_queries): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_values): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projections): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_normalisation_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (linear_layer_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_normalisation_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (self_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (linear_layer_keys): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_queries): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_values): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projections): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_normalisation_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (linear_layer_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_normalisation_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (self_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (linear_layer_keys): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_queries): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_values): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projections): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_normalisation_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (linear_layer_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_normalisation_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (self_attention_layer): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (linear_layer_keys): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_queries): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (linear_layer_values): Linear(in_features=128, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (projections): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (layer_normalisation_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward_layer): FeedForward(\n",
       "        (linear_layer_stack): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_normalisation_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_normalisation): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_layer): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "model.load_state_dict(torch.load(model_export))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc34d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model-01.pkl', 'rb') as f:\n",
    "     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4001618",
   "metadata": {},
   "source": [
    "# Use the model\n",
    "Finally, of course, we want to use the model. This simply generates new tokens for a given starting context. This is not ChatGPT, but at least it demonstrates the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fb9870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello WolHeloHerrlloWlrle\n",
      "rlddrl \n",
      "!H\n",
      "l\n",
      "Hello WoWWeoellre  r\n",
      "\n",
      "!ldr\n",
      "r odro reW\n",
      "\n",
      "Hello Wo lld Wl!o \n",
      "lrrHrl\n",
      "leeorHl!HHle\n",
      "Hello Wor\n",
      "r!eHlerdrH  dH  \n",
      "leH ooo W\n",
      "\n",
      "\n",
      "Hello WoWWr!errWd\n",
      "roHdeo dHldderWeedll\n",
      "Hello Woollo!!\n",
      "HH erH!W Hlor !leddl\n",
      "dd\n",
      "Hello Worddr  ldHd!!lelHdH!ld\n",
      " \n",
      "lrWrrl\n",
      "Hello Wo\n",
      "e rH !W \n",
      "ll oHrW !W orWl!r!\n",
      "r\n",
      "Hello Wood\n",
      "\n",
      "eddrdWleorWe!H\n",
      "Hede e!\n",
      "Hll\n",
      "Hello Wo\n",
      "rHol\n",
      "!!lW!ledHedeWeeoW  lo\n",
      "ee\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Hello Wo'\n",
    "if len(prompt) < sequence_length:\n",
    "  raise SystemError(f\"Prompt length ({len(prompt)}) cannot be smaller than sequence length: {sequence_length}\")\n",
    "context = torch.tensor(encode(char_table, prompt), dtype=torch.long, device=device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for iter in range(10):    \n",
    "    generated_chars = decode(reverse_char_table, model.generate(context.unsqueeze(0), max_new_tokens = 30)[0].tolist())\n",
    "    print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5a721",
   "metadata": {},
   "source": [
    "# Part 2 - Step By Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16b56a",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie1.png\">\n",
    "\n",
    "The following part will  look at the process inside the model, not the learning process. Meaning we are **not** going to deal with loss and optimizer. \n",
    "\n",
    "We will use a simple *text* containing 100 times the same chars: *ABCA*. \n",
    "\n",
    "First we need to encode it, simply using an integer char table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10a7503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 chars in our dataset: ABCAABCAABCAABCAABCA\n",
      "Char table: {'A': 0, 'B': 1, 'C': 2}\n",
      "Vocabulary size is: 3\n"
     ]
    }
   ],
   "source": [
    "data = 'ABCA' * 100\n",
    "\n",
    "chars = sorted(list(set(''.join(data))))\n",
    "vocab_size = len(chars)\n",
    "char_table = {char:index for index,char in enumerate(chars)}\n",
    "encoded_data = torch.tensor(encode(char_table = char_table, text = data), dtype=torch.long)\n",
    "\n",
    "print(f'First 20 chars in our dataset: {data[:20]}')\n",
    "print(f'Char table: {char_table}')\n",
    "print(f'Vocabulary size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd683a",
   "metadata": {},
   "source": [
    "Now we will take ```batch_size``` sequences of length ```sequence_length``` from random positions in the text. In this simple case this will result in **2 sequences** with a length of **4 chars** each. The output is a tensor with a shape of 2, 4 - 2 sequences, sequence length is 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b99b408c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 0, 1],\n",
       "        [0, 1, 2, 0]], device='mps:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4 # tokens per block/sequence\n",
    "batch_size = 2 # blocks per batch\n",
    "\n",
    "random_block_start_indices = torch.randint(len(encoded_data) - sequence_length, (batch_size,))\n",
    "            \n",
    "X_batch = torch.stack([encoded_data[index:index + sequence_length] for index in random_block_start_indices])\n",
    "X_batch = X_batch.to(device)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6977a0",
   "metadata": {},
   "source": [
    "FWIFW: We skip the `y_batch` for now. For that we just need to shift the position of the `X_batch` by e.g. 1 position to the right.\n",
    "\n",
    "Our first batch is ready. We could translate this back to readable chars, just to check what positions our previous function gave us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "962b5bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAAB', 'ABCA']\n"
     ]
    }
   ],
   "source": [
    "print([''.join(list(char_table.keys())[idx] for idx in sequence) for sequence in X_batch.cpu().numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9508663",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie2.png\">\n",
    "\n",
    "Now let's create the token embeddings, which is \"simply\" a float representation of length `embedding_size` for each char. As you can see, this increases the shape of or tensor: We have 2 sequences with a sequence length of 4 and each having 3 embeddings. If you read it from top to bottom, you get your letters/tokens for each sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a622340f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4070,  0.2609,  0.3285],\n",
       "         [-0.8823,  0.3972,  0.1231],\n",
       "         [-0.8823,  0.3972,  0.1231],\n",
       "         [ 0.5862,  0.2738, -0.7513]],\n",
       "\n",
       "        [[-0.8823,  0.3972,  0.1231],\n",
       "         [ 0.5862,  0.2738, -0.7513],\n",
       "         [ 0.4070,  0.2609,  0.3285],\n",
       "         [-0.8823,  0.3972,  0.1231]]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 3\n",
    "\n",
    "token_embeddings_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size).to(device)\n",
    "X_token_embeddings = token_embeddings_table(X_batch)\n",
    "X_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84bc720",
   "metadata": {},
   "source": [
    "The position embeddings works basically the same. This time we use a vector representing each position of the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "133176bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3], device='mps:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size).to(device)\n",
    "positions = torch.arange(sequence_length, device = device)\n",
    "positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5f428",
   "metadata": {},
   "source": [
    "And then we map our sequence to this position embeddings table. Now every position is represented by a vector with the length of `embedding_size`. Now the tensor consists of only one matrix. Apparently, because this tensor just represents the positions from 1 to 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf967c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2754, -0.0690, -1.2826],\n",
       "        [ 0.5533,  0.1064,  1.7657],\n",
       "        [-0.9466,  1.8032,  1.3748],\n",
       "        [ 0.0000,  0.0000,  0.0000]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_position_embeddings = position_embeddings_table(positions)\n",
    "X_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96998c0",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie3.png\">\n",
    "\n",
    "Finally we add up both embedding/matrices that we can send right away to into our Model. You still can see our tokens (aka chars) from top to botton, each one represented by a vector of floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "860bb5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1316,  0.1919, -0.9541],\n",
       "         [-0.3290,  0.5036,  1.8888],\n",
       "         [-1.8289,  2.2004,  1.4979],\n",
       "         [ 0.5862,  0.2738, -0.7513]],\n",
       "\n",
       "        [[-1.1577,  0.3283, -1.1595],\n",
       "         [ 1.1395,  0.3802,  1.0144],\n",
       "         [-0.5396,  2.0641,  1.7033],\n",
       "         [-0.8823,  0.3972,  0.1231]]], device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings = X_token_embeddings + X_position_embeddings\n",
    "X_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0559b17",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie4.png\">\n",
    "\n",
    "Wen the embeddings arrive in the Transformer block, they go straight to the first Multihead-Attention-Module where we first calculate `keys` and `queries` using `LinearLayers`. Every embedding is now represented by a vector, where the length is defined by `attention_head_size`. If you look closer, you may still recognize our tokens from top to bottom ;)\n",
    "\n",
    "While `keys` and `queries` serve a different purpose, the both come from the same mathematical background: a Linear Layer (with not bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7aaee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1988],\n",
      "         [0.0080],\n",
      "         [0.7045],\n",
      "         [0.2537]],\n",
      "\n",
      "        [[0.1745],\n",
      "         [0.1656],\n",
      "         [0.7288],\n",
      "         [0.0960]]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "tensor([[[ 0.0341],\n",
      "         [ 0.4429],\n",
      "         [ 2.2007],\n",
      "         [-0.1808]],\n",
      "\n",
      "        [[ 0.8383],\n",
      "         [-0.4488],\n",
      "         [ 1.3965],\n",
      "         [ 0.7109]]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# attention_head_size is set locally for training purposes, if we had multiple heads, we'd set this to embedding_size / num_heads\n",
    "attention_head_size = 1\n",
    "\n",
    "linear_layer_keys = nn.Linear(in_features=embedding_size, out_features=attention_head_size, bias=False).to(device)\n",
    "linear_layer_queries = nn.Linear(in_features=embedding_size, out_features=attention_head_size, bias=False).to(device)\n",
    "keys = linear_layer_keys(X_embeddings)\n",
    "queries = linear_layer_queries(X_embeddings)\n",
    "print(keys)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd5696",
   "metadata": {},
   "source": [
    "This step is called \"scaled dot-product\". Before we can multiply queries and keys, keys have to be transposed. Then we apply a \"scaling\" using square root. The result is now some kind of attention score representing, how our input tokens \"attend\" to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7123bfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1556,  0.2085, -0.2660, -0.0577],\n",
       "         [ 0.2727, -0.3654,  0.4661,  0.1011],\n",
       "         [ 0.5263, -0.7051,  0.8996,  0.1951],\n",
       "         [-0.2591,  0.3471, -0.4428, -0.0960]],\n",
       "\n",
       "        [[ 0.4145, -0.3154,  0.0124,  0.1627],\n",
       "         [-0.5585,  0.4249, -0.0167, -0.2191],\n",
       "         [ 0.5605, -0.4264,  0.0168,  0.2199],\n",
       "         [ 0.5943, -0.4521,  0.0178,  0.2332]]], device='mps:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_attention_scores = queries @ keys.transpose(-2,-1) * keys.shape[-1] ** -0.5\n",
    "scaled_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147398fc",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie5.png\">\n",
    "\n",
    "But we don't want them to attend to \"future\" tokens, that's why we need to drop some attention scores. To achieve that we use some PyTorch magic: The triangular matrix is a triangular made of 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5df3ebfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]], device='mps:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triangular_matrix= torch.tril(torch.ones(sequence_length, sequence_length)).to(device)\n",
    "triangular_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9e07d",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie6.png\">\n",
    "\n",
    "Now we mask the attention scores using this triangular matrix. This way a token  only attends to itself and all previous tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9473a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1556,    -inf,    -inf,    -inf],\n",
       "         [ 0.2727, -0.3654,    -inf,    -inf],\n",
       "         [ 0.5263, -0.7051,  0.8996,    -inf],\n",
       "         [-0.2591,  0.3471, -0.4428, -0.0960]],\n",
       "\n",
       "        [[ 0.4145,    -inf,    -inf,    -inf],\n",
       "         [-0.5585,  0.4249,    -inf,    -inf],\n",
       "         [ 0.5605, -0.4264,  0.0168,    -inf],\n",
       "         [ 0.5943, -0.4521,  0.0178,  0.2332]]], device='mps:0',\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_scores = scaled_attention_scores.masked_fill(triangular_matrix[:sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "causal_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c9d9c",
   "metadata": {},
   "source": [
    "And some housekeeping. Softmax results in actual probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "053e7d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6543, 0.3457, 0.0000, 0.0000],\n",
       "         [0.3644, 0.1064, 0.5293, 0.0000],\n",
       "         [0.2065, 0.3786, 0.1718, 0.2431]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2722, 0.7278, 0.0000, 0.0000],\n",
       "         [0.5119, 0.1908, 0.2972, 0.0000],\n",
       "         [0.3831, 0.1346, 0.2153, 0.2670]]], device='mps:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.softmax(causal_attention_scores, dim=-1) # (B, T, T)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cdd2d",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie7.png\">\n",
    "\n",
    "\"Dropping nodes\" is a common practice to prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e3e488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.0224, 0.5401, 0.0000, 0.0000],\n",
       "         [0.5693, 0.1662, 0.0000, 0.0000],\n",
       "         [0.3227, 0.5916, 0.2685, 0.3798]],\n",
       "\n",
       "        [[1.5625, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4254, 1.1371, 0.0000, 0.0000],\n",
       "         [0.7999, 0.2982, 0.0000, 0.0000],\n",
       "         [0.5987, 0.0000, 0.0000, 0.4172]]], device='mps:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.2\n",
    "\n",
    "dropout = nn.Dropout(dropout_rate)\n",
    "weights = dropout(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d9793",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie8.png\">\n",
    "\n",
    "Now, again using a \"simple\" Linear Layer without biases, we calculate values and multiply them with our weights. This results in weighted values or better: attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1127738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000],\n",
      "         [-0.1649],\n",
      "         [-0.1066],\n",
      "         [-0.1014]],\n",
      "\n",
      "        [[-0.4705],\n",
      "         [ 0.1984],\n",
      "         [-0.1553],\n",
      "         [-0.2430]]], device='mps:0', grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer_values = nn.Linear(in_features=embedding_size, out_features=attention_head_size, bias=False).to(device)\n",
    "values = linear_layer_values(X_embeddings)\n",
    "attention_scores = weights @ values\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecffd82f",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie10.png\">\n",
    "\n",
    "In the next step, within the MultiHeadAttention-Class, the weighted values of each head will now be **concatenated** and another dropout will be applied. We can skip this, as we only simulate one head here. The final result will be send back to the TransformerBlock, where it will be processed further. First step is a Normalisation Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e6580e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6491,  0.7635, -1.4127],\n",
      "         [-1.1116, -0.2014,  1.3129],\n",
      "         [-1.3952,  0.8975,  0.4978],\n",
      "         [ 0.9627,  0.4159, -1.3785]],\n",
      "\n",
      "        [[-0.7058,  1.4142, -0.7084],\n",
      "         [ 0.8869, -1.3974,  0.5105],\n",
      "         [-1.4026,  0.8579,  0.5447],\n",
      "         [-1.3846,  0.9415,  0.4431]]], device='mps:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "normalisation_layer_1 = nn.LayerNorm(embedding_size).to(device)\n",
    "normalised_attention_scores = normalisation_layer_1(X_embeddings + attention_scores)\n",
    "\n",
    "print(normalised_attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904954a",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie11.png\">\n",
    "\n",
    "And here comes the second important part of the Model: The FeedForwardLayer. Again, this is basically just a Linear Layer, a ReLu-Layer (remember part 02, binary classification?), another Linear Layer and again, a dropout layer. While the multi-head attention calculates relationships, this part refines individual characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71ad8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4651,  0.9240, -1.3892],\n",
      "         [-1.3505,  0.3116,  1.0388],\n",
      "         [-1.3701,  0.9884,  0.3817],\n",
      "         [ 0.7511,  0.6622, -1.4133]],\n",
      "\n",
      "        [[-0.8311,  1.4065, -0.5754],\n",
      "         [ 0.9631, -1.3784,  0.4153],\n",
      "         [-1.4033,  0.8532,  0.5501],\n",
      "         [-1.3939,  0.9037,  0.4902]]], device='mps:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feed_forward_layer = FeedForward().to(device)\n",
    "normalisation_layer_2 = nn.LayerNorm(embedding_size).to(device)\n",
    "\n",
    "refined_attention_scores = feed_forward_layer(normalised_attention_scores) # refine individual characteristics\n",
    "final_attention_scores = normalisation_layer_2(refined_attention_scores + normalised_attention_scores) # residual connection to mitigate the vanishing gradient\n",
    "\n",
    "print(final_attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27313a",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"gfx/Folie13.png\">\n",
    "\n",
    "Done that, we do the same same thing again in all the other Transformer Blocks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
